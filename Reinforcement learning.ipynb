{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e10bac",
   "metadata": {},
   "source": [
    "## 1 Problem Statement \n",
    "Traditional tree-based methods (e.g., Random Forests, CART, and Gradient Boosting Machines) perform poorly or become inconsistent in high-dimensional settings where the number of variables p greatly exceeds the sample size n (p ≫ n), and only a small unknown subset of variables (p₁ ≪ p) truly carries the signal (sparse signal). In such scenarios, these methods are plagued by excessive greediness or randomness in split selection, contamination by thousands of irrelevant or noisy variables, unstable variable importance measures, and—most critically—a lack of theoretical consistency guarantees as p → ∞.\n",
    "\n",
    "### State of the art in 2015\n",
    "By 2015, while Random Forests (Breiman, 2001) and Gradient Boosting (Friedman, 2001) dominated as state-of-the-art ensemble methods for moderate-dimensional data, they faltered in ultra-high-dimensional sparse regimes due to their reliance on random or greedy feature sampling, which amplifies noise pollution without built-in sparsity mechanisms. \n",
    "Emerging alternatives included Oblique Random Forests (ORF; e.g., Menze et al., 2011), which used linear combinations for splits to better handle correlations but lacked theoretical guarantees for p → ∞; Bayesian Additive Regression Trees (BART; Chipman et al., 2010), offering prior-based regularization for sparsity yet computationally intensive; and early embedding-based approaches like Sparse Local Embeddings for Extreme Classification (SLEEC; Bhatia et al., 2015), which projected high-dimensional features into low-dimensional spaces but were not tree-native and struggled with interpretability in tree ensembles. These methods improved on classical trees in specific niches (e.g., multi-label tasks or budgeted classification) but failed to provide a unified, theoretically consistent tree-based solution for sparse high-dimensional regression/classification, leaving a critical gap for applications demanding both accuracy and scalability.\n",
    "\n",
    "\n",
    "### Primary Users and Stakeholders:\n",
    "This problem disproportionately affects bioinformaticians and genomic researchers analyzing high-dimensional omics data (e.g., 20,000+ genes in microarrays for cancer biomarker discovery); medical imaging specialists processing sparse voxel data in fMRI or CT scans for disease diagnosis; fraud detection analysts in finance sifting through millions of transaction features to flag anomalies; and proteomics experts in drug discovery handling sparse mass spectrometry profiles for protein pathway identification. \n",
    "These users—often in academia, pharmaceuticals, healthcare, and fintech—face computational bottlenecks, unreliable feature selection, and regulatory demands for interpretable models, leading to delayed insights, higher costs, and missed opportunities in precision medicine and risk assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a58ec",
   "metadata": {},
   "source": [
    "| BO   | Objective                                                      | Model(s)                                      | Datasets                                  | Important Variables (examples)                                | Key Hyperparameters                                      |\n",
    "|------|----------------------------------------------------------------|-----------------------------------------------|-------------------------------------------|----------------------------------------------------------------|----------------------------------------------------------|\n",
    "| **BO1** | Define advanced learning strategies                            | **RLT (Reinforcement Learning Trees)**       | 10 UCI datasets + high-dimensional simulations | Cement, Age, Superplasticizer, LSTAT, RM, Sonar frequencies, Clump thickness, Cell uniformity | Embedded Extra-Trees (50 trees, depth=3), VI via OOB permutation, **pd=0.5**, linear splits (k=1–5), α=0.25 |\n",
    "| **BO2** | Compare classical models using advanced strategies            | **RLT** vs Random Forest, Extra-Trees, XGBoost, Oblique RF, BART | Same 10 UCI datasets (Concrete, Boston, Sonar, Breast Cancer, Wine, Parkinson, Ozone, etc.) | RLT selects **fewer and truer** variables than all baselines | ntree=500, nodesize=5, 5-fold CV tuning for all models, RLT with pd=0.5 and embedded model |\n",
    "| **BO3** | Increase model flexibility & adaptability (speed + variable selection) | **RLT with Linear Combination Splits + High Muting** | High-dimensional simulations (p=1000–5000), real datasets + added noise columns | **Only the true strong variables (p₁)** are selected — perfectly recovers 4–10 real signals even with 5000 noise variables | **pd=0.5** (mute 50% weakest per level), **k=3**, **α=0.25**, coefficient βⱼ = sign(corr)×√VIⱼ, convergence depends only on p₁ (theoretical proof) |\n",
    "\n",
    "### Reference\n",
    "Zhu, R., Zeng, D., & Kosorok, M. R. (2015). **Reinforcement Learning Trees**. *Journal of the American Statistical Association*, 110(512), 1778–1791. https://doi.org/10.1080/01621459.2015.1036994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59962dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
